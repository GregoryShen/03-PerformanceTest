# 张挺知乎live

## 引子-为什么要学性能测试

本次 live 主要介绍了性能测试的入门基础知识. 从理论开始讲到实践, 从原理上讲解性能测试到底是怎么回事. 与我们在网上能看到的大量讲性能测试工具的资料不同, 本 live 相较于怎么做更关注为什么这样做. 同时, 本 live 也关注并满足了最普遍的需求: 新人应该怎样在知识储备不足, 赶鸭子上架的情况下做性能测试? 会对这种情况下的处理方法做重点讲解

### 概述

1. 引子-为什么要学性能测试
2. 性能测试的基本原理
   1. Web性能测试的示意图
   2. 请求结果与监控结果
   3. 单次结果与统计结果
   4. 主要统计结果及其计算
3. 赶鸭子上架的性能测试怎么做
4. 性能测试场景设计策略
5. 赶鸭子上架的性能调优怎么做

### 为什么要学性能测试

1. 项目中往往不存在专职性能测试人员

2. 提高对系统的整体理解

3. 多维度思考系统质量,而非单纯功能角度

4. 转变自身角色

   我们作为测试人员并不仅仅是完成黑盒测试, 理解这个系统是怎样工作的

### 性能测试想做什么 - 尽可能模拟真实业务场景

UI泛指浏览器、移动app、桌面app、无图形界面的各种接口等

服务端泛指处理请求的服务器及他背后可能存在的很多很多服务器

上图是一个简化过的web性能测试的示意图,性能测试时尽可能用接近生产环境的专门环境来测试,即使如此仍然不能做到与生产环境完全相同,因此只能做估算和参考.

## 性能测试的基本原理

1. 模拟各种业务场景下的负载压力

   负载压力就是比如说我发很多请求或者说打开很多网页这就是负载的压力, 或者有很多用户同时做登录. 看你具体业务, 只不过是单个人做这种业务可能负载压力不大, 但是很多人做这个业务负载压力就会变大, 所以就要用工具去模拟

2. 观察压力下服务端业务逻辑处理是否正常

   单用户做一个操作可能是正常的, 假设百万个用户同时做这个操作可能业务逻辑处理可能就会出错, 所以就是观察在压力下服务端业务逻辑处理是不是正常, 压力可能是不同的压力下要观察

3. 观察压力下服务端资源使用状况是否吃紧

   服务端有什么资源, 比如服务器是一台电脑, 电脑有CPU、内存、硬盘、网络, 这些都是有资源的一些指标, 比如CPU使用率,一开始是1% 2%,用户数量上来了,压力上来了,可能是50% 60% 这个时候同时它的业务逻辑, 导致业务逻辑可能会出错 

4. 改变模拟的压力来重复上述观察

   通过改变模拟的压力, 可以模拟小压力或者大压力来重复上面的观察.可以改变小压力

别的观察主要有服务端软件层次的观察. 当然这个也可以包含在业务逻辑里面.

一个简单的性能测试的基本流程:得到性能测试结果

性能测试结果由<u>请求结果</u>与<u>服务端监控结果</u>组成

<u>请求结果</u>包括响应时间、返回值等**<u>单次请求的响应里找到的结果</u>**以及他们对应的<u>统计结果</u>

请求结果: ==通过解析服务端对请求的响应直接得到的结果==, 如响应时间、响应的http状态码、响应体内容、单个响应包含的数据量等.此外==还可以通过自定义逻辑间接得到一些复杂的请求结果==.(比如一个无论如何都返回200的接口, 其真实成功与否由响应体内某字段决定)

有一些请求结果是要做一些比较复杂的逻辑运算的. 

==请求结果是一个一个互相独立的结果, 每个请求有自己的请求结果==

自定义的逻辑, 然后按照开发, 一些请求结果是要做一些比较复杂的逻辑运算的. 互相独立的结果. 比如测试发了1000个请求结果, 还有500个是失败了. 这就是请求结果, 他是一个一个互相独立的测试结果.

<u>服务端监控结果</u>包括<u>服务端各项资源使用率及数据量</u>等统计结果

服务端监控结果:==一段时间内==的CPU的负载, CPU使用率、网络数据传输量、内存的占用率、IO数据量==等指标变化的情况==

发包的工具, 用来发包, 发给右边的服务器, 服务器给他一个返回值, 可以解析出来响应时间、错误率等指标, 右边服务端上我们装了一个监控器, 也有可能是云平台提供的, 比如 C

服务端监控结果是随着时间线不断变化的指标, 这一段时间内所有请求和响应共同造成了这个结果. 图中例子在11:30附近CPU占用率100%,表示到了性能瓶颈, 出图原理在时间线上各个时间点x坐标标识出对应的指标y坐标, 然后连成线, 所以图中11:00 到11:15中间由一小段同一个x上两个y上不可能的

### 单次结果与统计结果的区别举例:

**单次结果**: 调用正确/调用错误

**统计结果**: <u>持续调用n小时或n</u>次, 其中调用正确比例`x%`, 叫做<u>正确率</u>`x%`或错误率`(1-x%)`

以此类推, 单次结果的维度可以很多, 不局限于正确率错误率, 比如, 单次结果中返回的http代码是200, 404, 500, 对应的统计结果是200比例x%, 404比例y%, 500比例z%, 要统计哪些单次结果?由具体业务决定, <u>统计结果还包括通过对统计结果进行计算得到的新的统计结果.</u>



### 主要统计结果及其计算

**(总)并发用户数**: 同一时间在系统上的用户数量, 这些用户可能分布在不同的功能模块或页面上

**(总)并发请求数**: 同一时间在系统上的用户同时向服务器作出的请求数量, 这些请求也可能分布在不同的功能模块或页面上.

同一时间一个用户可能只给系统发一个请求, 也可能给系统发100个请求, 所以, 下次有人告诉你系统要支持多少并发的时候, 问清楚上并发用户数还是并发请求数(这两个数字可能差很多)

**吞吐量(平均吞吐量)**: 吞吐量表示待测应用对业务的支持量, 以TPS或QPS为单位, 表示每秒钟能处理的请求数

**平均响应时间**: 一些请求从发起到收到服务端响应所需的时间的平均数
$$
一段时间内的请求平均响应时间 = \dfrac{单个请求的响应时间之和(秒)}{这段时间内服务端处理掉的总请求数(个)}
$$

$$
一段时间内的平均吞吐量 = \dfrac{这段时间内服务端处理掉的总请求数(个)}{时间(秒)}
$$

> 单位名称是(TPS, transaction per second) 或 (QPS, Query per second)

设这段时间内服务端处理掉的总请求数(个)为n, 这段时间为t(秒), 一段时间内的请求平均响应时间为Tv(秒), 一段时间内的平均吞吐量为Qv, 单个响应的响应时间为Ts, SUM为求和函数. 则 

$$
Tv = \dfrac{\sum{Ts}}{n}
$$

$$
Qv = \dfrac{n}{t}
$$

由公式1与公式2推导出:

```
Qv = SUM(Ts)/Tv/t
```

`Qv = SUM(Ts)t/Tv` — 公式3

由公式3得: 平均吞吐量`Qv`与平均响应时间`Tv`成反比.

即: ==平均响应时间越长, 则平均吞吐量越小==.

这个推导是符合逻辑的, 当服务器硬件性能固定时:

1. 假设服务端只提供数据量极小的静态页面, 没有任何业务逻辑, 只需要处理0.1毫秒, 那么服务端可以每秒钟同时处理极大数量的请求, 那么吞吐量就高
2. 假设服务端收到每个请求都要处理1小时才给响应, 那么服务端每小时才能处理1个请求, 吞吐量就极低.

这个推导结果主要说明了两个问题:

1. 性能测试关键指标吞吐量与具体业务密切相关, 因此设计性能测试不能脱离具体业务.这一点会直接影响性能测试工具的设计与挑选
2. 不同业务的服务端, 无法通过对比吞吐量来判断谁的性能强.业务耗时小的服务器的吞吐量显然会比业务耗时大大服务器的吞吐量高.同理,并发请求数高也不能代表牛逼. 因为只有项目参与人员才知道这高并发背后的业务到底是简单业务还是复杂业务. 简单业务的高并发搞不好还是复杂业务的低一点的并发更难、要求更高. 因此有些公司在jd里吹牛逼说这个项目要处理高并发多厉害能让人学到东西, 到底有多厉害还是很难说的. 另外, 得益于基础设施的技术发展, 云计算平台的发展, 高并发也越来越容易实现了.

错误率: 一段时间内出错请求个数(个)/总请求数(个)

什么叫出错的请求?

1. 没有任何响应的请求 — 超时
2. 有响应,但卡了很久才给响应 — 超时
3. 有响应,响应代码不是预期值 — 断言错误
4. 有响应,响应代码是预期值.但响应体内有数据不是预期值 — 断言错误

超时请求的影响:

==公式1==:

一段时间内的请求平均响应时间 = 单个请求的响应时间之和(秒) / 这段时间内服务端处理掉的总请求数(个)

因为公式1中单个请求的响应时间之和(秒)很不幸地包含了超时请求, 因而超时请求的响应时间会导致平均响应十斤啊不能反映大多数请求的响应时间.

如下例子:

10个响应中9个都是1秒钟拿到响应, 1个卡了99秒才拿到响应, 那么根据公式1, 平均响应时间 = (9*1+99)/10=10秒,但实际上大多数请求(90%的请求)的响应时间都是1秒, 因此引出了新的统计结果: ==90%平均响应时间==

**<u>90%平均响应时间</u>**: 从平均响应时间计算时所统计的那些请求里, 去掉响应时间最长的10%的请求后, 剩余的请求计算出来的平均响应时间.

90%平均响应时间有效地排除了个别超时请求对平均响应时间的影响, 因此而使数据更接近真实业务场景的反映. 同理还有80%平均响应时间, 70%…等等

在jmeter等一些性能测试工具中, 往往可以直接显示x%平均响应时间这种指标, 可见这个指标的通用性.

**<u>数据吞吐量/平均传输带宽</u>**: 这个指标用于计算服务端的数据传输量

在一些重数据传输的性能测试场景中, 我们需要监控和计算这个指标

顺便讲一下数据传输的单位: kbps(千比特每秒), 又称千比特率, 指的是数字信号的传输速率, 也就是每秒钟传送多少个千位的信息(k表示千, kb表示的是多少千个位), 也可以表示网络的传输速度, 为了在直观上显得网络的传输速度较快, 一般公司都使用kb(千位)来表示, 如果是大写的B的KBps, 则表示每秒传送多少个千字节.

1kByte/s=8kbit/s(一般简写为1KB/s=8kb/s)

根据业务的不同, 单位名称会变大, 比如:

1MB=1,024KB=1,024 \* 1,024b=1048576b

MB上面还有GB, TB, PB

我曾经测过跨过数据库同步业务的压测项目, 要求平均传输带宽达到GB级.

具体要求多少传输带宽, 我们也是根据**实际数据库大小/能接受的时间**来算出来的.

## 赶鸭子上架的性能测试怎么做

马上就要“我”搞性能测试, 但是“我”没做过, 也不会做性能测试, 公司里也没有人会做, 时间紧迫了, 现在怎么办?

首先根据性能测试基本原理, 我们要模拟各种业务场景下的负载压力. 因此:

1. **<u>90%平均响应时间</u>**. 比如有些不常用的用户注册场景可能不需要做压测.
2. <u>由熟悉业务的人士, **估算**其并发用户数, 再由熟悉业务的人士和熟悉技术实现的人士一起**估算**</u>这个并发用户数的并发请求数, 注意, 很多系统的负载都有**平时**和**高峰**的区分, 比如12306平时用的人不太多, 但春运订票高<u>峰时并发用户数与并发请求数都会暴涨</u>.假设系统平时有一千人同时在线, 每天高峰时段有三千人现在活每年高峰月份的高峰时段有五千人同时在线
3. 然后就要考虑这次性能测试所用的环境大概能支持多少用户, 这个环境和生产环境的区别: 假如这个环境能支持x个用户, 那么生产应该可以支持y个用户. 最好估算出这样的x和y的关系. 然后, 假设x个用户的请发请求数数n,测试中还要留出缓冲风险的时间, 那么希望达到的并发请求数可以是n的1.1倍到1.5倍, 具体几倍后面测完了再说. 最后就是测出来支持xxx用户, 领导来说拍板说行不行, 够不够, 重点是**建立这个性能测试环境可能需要开发和运维等人帮忙.** 尽量搞个独立的环境.
4. 然后, 对1里列出的业务场景做具体的测试场景设计.
5. 对1里列出的业务场景做具体的测试脚本实现. 脚本实现最简单的就是jmeter录制脚本, 然后做参数化, 加检查点. 照着jmeter用户手册学习一两天就能上手.
6. 对1里列出的业务场景做具体的测试脚本执行. 执行的时候需要做好服务端监控. 还记得我们前面列过需要监控的指标吧, 包括但不限于那些指标. 因为是赶鸭子上架不会做, 怎么办, 找运维活开发一起做, 你这边跑脚本, 那边让他们盯着看指标正常不正常, 自己盯着当然也行.
7. 进入调优环节.
8. 给出测试报告, 汇总整个测试过程, 给出各种性能指标.

## 性能测试场景设计策略

1. 基线测试baseline testing: 

   单用户跑一遍, 此时的性能测试结果会作为后续的对比依据.

2. 性能测试performance testing: 

   逐步增加并发请求数比如10个,20个, 50个,80个,130个,得到不同并发请求数情况下的性能测试结果变化趋势图.这个图里常常可以发现性能瓶颈.比如用户数到某个值时突然性能指标下降了很多, 错误率大幅上升了, 那就需要进一步分析是软件的问题还是硬件的问题, 还是环境的问题等. 这里, 并发请求数要一直驾到之前预估过的测试环境需要支持的最大数字.

3. 压力测试load testing: 

   使用超过系统设计的最大用户数, 看看待测应用会不会崩溃, 崩溃后会怎么样, 有没有隐患和风险等, 比如数据大量丢失之类, 崩溃后无法恢复等都是大问题.

4. 稳定性测试stability testing

   在较高负载下, 做较长时间的测试, 来观察系统的稳定性

5. 静态测试static testing

   对某静态网页做性能测试, 保证错误率低且吞吐量高, 用来验证网络硬件设置正确.

此外还要在每种场景都考虑测试预期结果:

之前说过的各种测试结果和其计算结果需要在这里设计好, 其值的合理范围.

比如, 请求正确率>95%, 90%平均响应时间< 1000ms等

还要设计每个场景的具体数据. 比如稳定性测试准备做两次, 一次做72小时, 使用性能测试时测出来的能支持的最大用户数的80%. 另一次做140小时, 使用40%的最大用户数.

## 赶鸭子上架的性能调优怎么做

首先, “一个人”是做不了的,除非你是传说中的“性能专家”.

然后, 我们需要团队合作, 测试人员来牵头, 找开发、运维、架构、DBA、基础设施提供商的技术支持等角色一起做调优.

如下图, 测试人员最基本的就是做到“那个谁,你再跑一遍测试”的时候, 可以点一下测试脚本让它再运行一遍, 然后把问题留给专业的人来分析和优化.



## 项目实例

### 某小型第三方支付平台压测

#### 需求

##### 业务背景:

公司准备上线一个新的第三方支付平台网站, 用户基本没有, 全靠公司上头的集团在内部员工中推广...此外集团还在某城市有一定的资源, 能借助那些资源在这一个城市做一点推广. 但总之用户数最多也就是几千到一两万左右, 其中同时在线的用户, 那就估计顶多几百.

#### 设计思路

##### 场景设计策略: 

只打算做静态测试, 基线测试和性能测试

##### 业务场景选择: 

其它接口都不测, 只测登录和支付

##### 服务端监控: 

由运维使用商业监控软件负责, 压测全程运维肉眼观察.(后来因为该项目买的服务器过于高端, 服务器硬件全程无压力, 服务器买的过于高端据说是为了把集团给的钱先用掉, 以免来年降低预算.)

脚本检查点:

只看表示登录成功和支付成功的字段的值

数据准备:

预先写脚本注册了几百个账号, 然后把账号和密码写在文本里, 通过参数化传给jmeter.

脚本:

用jmeter花5分钟时间录的.录下来去掉那些多余的无关请求, 改一下参数改成从文本里读.然后加个检查点.

#### 工具选型

jmeter, 理由: 纯http接口, 简单无脑

#### 遇到的问题和总结

我的第一个压测项目, 第一次压静态测试直接挂了, 然后开发修, 修不好, 找硬件技术支持一起修. 修好了.第二天, 低负载性能测试时就出了错. 同时服务端监控显示毫无压力. 架构师估计问题在java web 服务器, 然后开发修, 修好完事了. 具体啥原因不知道, 赶鸭子上架第一次.

### 某第三方artifactory服务器上传下载大文件压测

#### 需求

##### 业务背景:

项目属于某devops项目的子项目, 打算使用某个第三方artifactory服务器. 这个服务器用来存放build踹的jar包及其依赖. 此外, 这个服务器后台用了某个第三方云平台的存储技术. 需求中的大文件指用户同时上传和下载1GB以上大小的大量文件

```mermaid
graph LR
A(用户文件) -->B(待测服务器) -->C(云平台存储)
```

#### 设计思路

##### 场景设计策略:

只打算做基线测试和性能测试

##### 业务场景选择:

总共两个接口, 一个上传一个下载

##### 服务端监控:

系统级监控分两块, 一块由云平台提供, 云平台自带服务端监控, 另一块, 我自己登录到待测服务器上, 敲民营展示当前的系统上的http连接数, 因为上传下载都是用http协议传文件.

还不够, 还要应用级监控, 通过我自己登录到待测服务器上, 打开待测应用到日志文件的方式, 观察其中日志是否报错.

#### 工具选型

shell脚本直接做并发.

因为业务是大文件上传下载, 慢的很, 根本不需要什么精确的高并发, 业务上我们也只要支持到最多100个用户同时上传下载大文件, 我每个脚本做10个进程, 然后搞10台负载机来模拟这100个用户.

##### 为什么不用jmeter?

根本没必要, 首先它只有两个http接口, 每次还只要测1个, 然后jmeter这么麻烦, 我只要传个文件而已,犯不上搞个图形界面工具弄脚本

#### 遇到的问题和总结

一开始忘了做md5校验. 脚本上要做md5确认来确保上传或下载真的成功且完整, 还要搞并发, 因此shell脚本后面有点复杂了.

一开始没意识到瓶颈上负载机的存储容量, 搞得负载机硬盘太小, 后来加大了.

一开始第一次搞大文件就失败了, 然后查待测软件服务端日志, 查第三方服务器的相关博客, 发现它后台是用tomcat服务器, 于是再ssh登录到待测服务器去修改tomcat配置, 把jvm改大, 把模式改成支持并发的模式, 再把这个第三方服务器自己的后台配置里关于如何使用云存储的配置做了修改, 把配置改大.

然后就可以了, 主要优化点都在待测服务器的服务端配置上.

### 某数据库百万级数据同步性能测试

#### 需求

##### 业务背景:

某公司在某国有大型数据库集群内有288万条数据, 且数据在不断变化, 在跨国大洋的另一个国家有用户要访问这些数据, 因此设计了一个本地数据库, 采用一次性同步+后续实时同步的方案来完成数据同步.

```mermaid
graph LR
A(某国用户) -->B(某国的数据库) -->C(隔海的数据库)
```

#### 设计思路

##### 场景设计策略:

基线测试, 性能测试(一张小表同步, 一张表同步, 全部同步)、稳定性测试(仅针对实时同步)

##### 业务场景选择:

总共两个业务场景, 一次性同步和实时同步

##### 服务端监控:

我自己写脚本连接到某国数据库上, 反复查询待测表的数据行数并打印时间戳和数据行数, 然后脚本我搞到jenkins上, 我就让它自动监控着, 事后分析jenkins日志即可.

#### 工具选型

首先我不需要jmeter, 数据同步不是由http请求触发的, 一次性数据同步是由我们开发的某应用触发的, 而且只需要出发一次, 所以我直接人肉点一下触发就行了. 当然这个出发我也做成了脚本, 放到jenkins上.

监控也不需要云平台的监控, 因为服务器撑得住, 我只管查一下表里有多少数据就行了, 所以我只写了一个监控脚本放到jenkins上让它自动跑.

#### 遇到的问题和总结

首先源数据库很麻烦, 是oracle, 目标数据库则是postgresql, 同步要通过oracle公司的一个商业工具来做, 而这个工具的配置和使用非常复杂, 安装也麻烦, 而要测这个需求, 必须多少会一点, 所以就要现学, 时间又紧迫.

这个项目的压力生成方式很独特, 是通过对商业同步工具做配置, 配置并发同步进程和每个进程同步文件大小来决定产生的压力大小的. 在压测前期, 常常是直接同步失败, 同步过程卡死. 这是由于测试环境和压测环境的不同, 造成了我和开发在测试环境的准备的脚本在测试环境上能跑, 在压测环境(类似于准生产环境)上就挂掉. 最终我们在压测环境搞了好几周才搞通.

另外这个商业工具做数据同步的机制也很独特. 它是先把数据源从远端同步到本地(目标数据库的本地), 然后从本地数据源把数据插入目标数据库, 我们最初搞的同步280万条数据也就分两步, 第一步同步数据源只花了1小时15分, 而插入数据花了4小时42分, 每秒插入了1657条数据, 当时配置的并发数是5, size是1000M, 当然后续还继续做了优化, 但其实也没太多必要, 因为用户可以接受这个时间.

最后还有个问题是我们在做压测的过程中, 依赖的是第三方云平台的业务发生了更新, 发布了可以直接帮我们简化很多步骤的新功能, 于是我们的项目架构跟着做了修改, 这也影响了压测的进度(体现在环境搭建脚本的更改上)

还有实时数据同步测试采用了半自动化测试的方式来做, 一边在源数据库插入数据, 一边在新数据库用脚本做查询. 半自动化测试, 在性能测试中是经常使用的方法.



# [公众号 性能测试基础](https://mp.weixin.qq.com/s?__biz=MzUyMTM4MzYyMw==&mid=2247483743&idx=1&sn=683d5a74a8e4104775b108477d5f92a5)

主要讲针对服务器端的性能测试，其他代码级性能测试、前端性能测试等属于比较细分的领域，就不说了。

## 性能测试想做什么

首先，性能测试想做的事情，类似下图：



这是一个简化过的关于 Web 应用的服务端的性能测试示意图，性能吃想要模拟真实业务场景。当一个应用上线，有很多用户通过客户端访问服务端。他们把请求通过用户界面发送给了服务端，于是在服务端接收到了大量请求，如果用户数很多，那么服务端有可能承受不了这种压力，进而崩溃，严重的可能导致大量经济损失。

性能测试则是希望通过提前模拟这种压力，来发现系统中可能的瓶颈，提前修复这些 bug，减少服务器宕机的风险。此外，性能测试还可以用来评估待测软件在不同负载下的运作状况，帮助管理层做一些决策。比如早期有的管理者会希望通过性能测试来评估需要买几台服务器。但这种做法随着云计算的普及已经过时。在云计算平台上硬件资源可以弹性获取，自动调整。云计算也会对性能测试的方式产生影响，包括服务端资源监控在内的很多工作，未来都可以交给云计算平台了。

## 性能测试的基本流程

一个简单的性能测试的基本流程：得到性能测试结果

然后，这只是一轮性能测试的开始，得到性能测试的结果之后，还有很多工作：

可以说，这里的调优过程才是性能测试的技术含量所在。这可能也是性能专家这个角色存在的意义之一。而测试人员通过做性能测试，进阶称为性能专家，也是一种发展方向。

调优的话，主要看调优小组推测的瓶颈在哪里，比如推测瓶颈在数据库，可能让 DBA 调整一下数据库配置，然后再测一遍。推测瓶颈在第三方提供的设备上，那就咨询一下第三方的技术支持，然后调整一下配置再测一遍。这个调优过程，可能比较长。

从以上两图可以看出，在整个性能测试过程中，测试人员能做的工作其实并不多，且大多数属于第一阶段的低技术含量工作。主要集中在脚本录制或编写、性能监控器的配置和数据的收集上。后者还会受到云平台普及的冲击，很可能以后不用你做任何工作就能简单收集到想要的资源使用情况数据。而在第二阶段调优时性能调优小组的介入使得测试人员存在感进一步下降。其中水平较低的测试人员将沦为脚本执行员和脚本修改操作员。

## 性能测试的前期准备

分析业务场景            –>     得到预估吞吐量

分析测试策略             –>    设计测试场景

分析生产环境和预算 –>     搭建测试环境，并建立估算模型

选择测试工具             –>     编写测试脚本

吞吐量：表示待测应用对业务的支持量，以 TPS 或 QPS 为单位，表示每秒钟能处理的请求数，需要分析业务场景来估算吞吐量。

例：小明办了一个电影网站，每天有2000个用户来上看电影，假设每个人都只看一部电影，每部电影需要播放两小时。每个用户每看一部电影会向待测应用运行的服务器发送10个请求，其播放的电影数据存储和传输在第三方，本题中不需要考虑。每天晚上7到9点为黄金时间，在每天的用户中会有一半人选择在这个时间点上来看电影。请计算该网站的平均吞吐量和峰值吞吐量。

UV = 2000

Ts = 2 h
$$
平均吞吐量 = \dfrac{一段时间内总的请求数}{一段时间}
$$
那就是
$$
Qv = \dfrac{2000 * 10}{24 * 60 * 60}
$$

$$
Qm = \dfrac{1000 * 10}{2 * 60 * 60}
$$

最后算得 Qv = 0.02, Qm = 1.39

除了这些，还有很多估算。比如故意夸大一点，乘以一个估算系数之类的。总的来说，算出来的吞吐量不一定很精确。

虚拟用户数：性能测试要模拟的用户数量

性能测试结果：包括响应数据和性能指标。

响应数据相关的常见指标有：90%平均响应时间、错误率等

性能指标包括 CPU，I/O, Memory, Network 四个方面，每个方面都有几个指标

### 测试策略

代表设计这些场景的依据，在这里主要有以下几种：

1. 基线测试，又叫基准测试

	测一下单个用户做主要业务流程的场景。其性能测试结果会作为对比依据。

2. 性能测试

	逐步增加用户数，比如10个，20个，50个，80个，130个，得到不同用户数情况下的性能测试结果变化趋势图。这个图里常常可以发现性能瓶颈。比如用户数到某个值突然性能指标下降了很多，错误率大幅提升了。那就需要进一步分析是软件的问题还是硬件的问题，还是环境的问题等

3. 压力测试

	使用超过系统设计的最大用户数，看看待测应用会不会崩溃，会怎么样，有没有隐患和风险等

4. 负载测试

	在比较高的用户数情况下，较长时间执行测试，观察系统在较高负载较长时间下的稳定性

5. 根据性能测试的负载生成器所在位置，还可以分出本地同网络的测试和跨网络的测试

6. 还有当下比较时髦的全链路测试，其实就是把真实环境划出一块来做测试。这样来解决估算不准的问题。

关于分析生产环境和建立估算模型，可以去搜阿里研究员对全链路压测的演讲。

其他在前期要做的事情，还有确定测试通过标准，比如吞吐量要达到多少，错误率低于多少，响应时间要多块，服务器 CPU 占用率要多少以下等等。

以及确定测试环境用什么设备，怎么搭建等等

## 性能测试的工具及相关学习建议

主流性能测试工具： Jmeter

监控服务器指标方面， jmeter 有插件支持，也可以用 Linux 命令行工具做。也有人用商业工具做。总之要把服务器性能指标和请求响应时间两组数据的时间线对上，让我们可以看到什么时候发送了多少请求，响应时间多少，错误率多少，此时服务器负载又是多少。

还有blazemeter的博客，比如性能测试指标怎么分析，看到性能指标图在什么地方出现拐点最可能的瓶颈在哪里。

还有微软MSDN的： Performance Testing Guidance for Web Applications

# [公众号 常见性能测试指标](https://mp.weixin.qq.com/s?__biz=MzUyMTM4MzYyMw==&mid=2247483934&idx=1&sn=56adbe1c86b4dc98a444d8d835eddce6)

## 吞吐量（平均吞吐量）

吞吐量表示待测应用对业务的支持量，以 TPS 或 QPS 为单位，表示每秒能处理的请求数.

有一个很模糊的概念“并发数”, 似乎和吞吐量有关:

比如经理说, 这个系统要支持 2000 并发, 那么这个要怎么理解, 并发和吞吐量是同一个东西吗?

不一定, 并发数可能是:

## （总）并发用户数

同一时间在系统上的用户数量, 这些用户可能分布在不同的功能模块或页面上

## （总）并发请求数

同一时间在系统上的用户同时向服务器做出的请求数量, 这些请求也可能分布在不同的功能模块或页面上.

所以整个时候就问清楚经理, 是2000并发用户数, 还是2000并发请求数, 后续可以根据不同的回答来设计不同的测试场景.

再说一下为什么这里提到了测试场景.

因为在测试一个系统的性能或者说吞吐量时, 离不开具体业务场景. 在解释为什么之前, 先看这个:

## 平均响应时间

一些请求从发起到收到服务端响应所需的时间点平均数.

说下为什么离不开具体业务场景, 请看公式1:
$$
一段时间内的平均吞吐量 = \dfrac{这段时间内的总并发请求数}{这段时间的平均响应时间}
$$


比如获取一个静态图片, 响应时间就短, 那么根据公式1, 单位时间内的请求的平均响应时间越小, 其平均吞吐量就越高. 而如果是请求一个需要服务端做一定计算的资源, 那么响应时间就长. 自然按照公式1, 就会发现吞吐量降低了.

也就是说响应时间和吞吐量成反比, 因此讨论系统的性能时离不开响应时间, 也就离不开具体的业务场景.

在实际测试工作中, 我们会采用逐步加压的方式, 一步一步提高虚拟的总并发用户数, 并观察其响应时间变化, 因为响应时间和吞吐量成反比, 那么我们观察响应时间的时候其实也就是在观察吞吐量的变化.

压力较低时, 吞吐量和虚拟的总并发用户数可能成正比, 当用户数逐步增加上来之后, 可能吞吐量的增加速度会逐渐下降. 这是因为压力上升后, 系统处理请求能力下降, 平均响应时间加长. 直至某一个点开始, 吞吐量不再上升, 反而下降. 这就是系统处理能力的瓶颈了.

而在这个吞吐量上升的过程中， 我们会观察到还有一个数字有可能上升，那就是：

## 错误率

一段时间内出错的请求在总请求数中的占比。

对于错误率的容忍度，取决于不同的系统需求。那么一般错误又分情况：

### 有返回值的错误

这里又要分为 HTTP 请求之类的错误，还是业务上的错误。这些可能出现错误的值需要在测试脚本里做校验，或者说断言。

### 没有返回值的错误

也就是超时。

有些请求会超时，那么不但会导致错误率的出现还会影响平均响应时间。因为：
$$
平均响应时间 = \dfrac{所有请求所需总时间}{所有请求总数量}
$$
而个别超时的请求会严重增加请求所需总时间。因此，对于平均影响时间就会有更多细分，比如

## 90% 平均响应时间

从计算平均响应时间的那些请求里去掉最慢的10%之后重新计算平均响应时间。

显然，使用90%平均响应时间，因为去掉了出错超时的那些请求，使得得到的数据更加接近真实值。

### 补充关于90% 平均响应时间的说明

来源: [jmeter性能测试重要指标以及性能结果分析](https://www.huaweicloud.com/articles/f845611bf6241f674c13604ba7a562fb.html)

#### 描述性统计与性能结果分析

疑惑点: 90% 响应时间是什么意思? 这个值在进行性能分析时有什么作用?

为什么要有 90% 用户响应时间? 因为在评估一次测试的结果时, 仅仅有平均事务响应时间是不够的. 原因是平均事务响应时间满足了性能需求, 但这并不能表示系统的性能已经满足了绝大多数用户的要求.

假设有两组测试结果, 响应时间分别是{1, 3, 5, 10, 16} 和 {5, 6, 7, 8, 9}, 他们的平均值都是7, 你认为哪次测试的结果更理想? 假如有一次测试, 总共有100个请求被响应, 其中最小响应时间为0.02秒, 最大响应时间为110秒, 平均事务响应时间为4.7秒, 你会不会想到最小和最大响应时间如此大的偏差是否会导致平均值本身并不可信?

为了解答上面的疑问, 我们先来看一张表:

|   CmdID   | NUM  | MEAN | STD DEV | MIN  | 50th |
| :-------: | :--: | :--: | :-----: | :--: | :--: |
| Home Page |  99  | 4.53 |  1.47   | 3.33 | 4.08 |
|  Page 1   | 100  | 2.94 |  0.85   | 2.26 | 2.59 |

每列的含义如下:

CmdID: 测试时被请求的页面

NUM: 响应成功的请求数量

MEAN: 所有成功的请求的响应时间的平均值

STD DEV: 标准差

MIN: 响应时间的最小值

50th(60/70/80/90/95th) 如果把响应时间从小到大排序, 那么50%的请求的响应时间在这个范围内. 后面的也是一样的含义.

总结:

1. 90% 用户响应时间是可以设置的, 可以改为 80% 或 95%

2. 对于这个表, LR没有提供, 可以使用 Excel 中的 PERCENTILE 函数算出不同百分比用户请求的响应时间分布情况.

3. 从上面的表中来看, 对于 Home Page来说, 平均事务响应时间(MEAN)只同70%用户响应时间一致.也就是说假如我们确定 Home page的响应时间应该在5秒内, 那么从平均事务响应时间来看是满足的, 但是实际上有10%~20%的用户请求的响应时间是大于这个值的, 对于Page 1也是一样, 假如我们确定对于Page 1的请求应该在3秒内得到响应, 虽然平均事务响应时间是满足要求的, 但是实际上有20%~30%的用户请求的响应时间是超过了我们的要求的.

4. 可以在 95th 之后继续添加 96/97/98/99/99.9/99.99th, 并利用 Excel 的图表功能画一条曲线, 来更加清晰表现出系统响应时间的分布情况. 这时候你也许会发现, 哪个最大值的出现几率只不过是千分之一甚至万分之一, 而且 99% 的用户请求的响应时间都是在性能需求所定义的范围之内的

5. 如果想用这种方法来评估系统的性能, 一个推荐的做法是尽可能让你的测试场景运行的时间长一些, 因为当你获得的测试数据越多, 这个响应时间的分布曲线就越接近真实情况

6. 在确定性能需求时, 你可以用平均事务响应时间来衡量系统的性能, 也可以用 90% 或 95% 用户响应时间来作为度量标准, 他们并不冲突. 实际上, 在定义某些系统的性能需求时, 一定范围内的请求失败也是可以被接受的

7. 上面提到的这些内容其实是与工具无关的, 只要你可以得到原始的响应时间记录, 无论是使用哪种工具, 你都可以用这些方法和思路来评估你的系统的性能.

   聚合报告中:
   $$
   吞吐量 = \dfrac{完成的 transaction 数}{完成这些 transaction 数所需要的时间}
   $$

   $$
   平均响应时间 = \frac{所有响应时间的综合}{完成的 transaction 数}
   $$

   $$
   失败率 = \dfrac{失败的个数}{transaction数}
   $$

   总的来说, 对于 jmeter 的结果分析, 主要就是对 jtl 文件中原始数据的整理

8. TestPlan: 是整个 Jmeter 测试执行的容器

9. ThreadGroup: 模拟请求, 定义线程数、Ramp-Up Period、循环次数

10. Step1: 循环控制器, 控制 Sample 的执行次数

11. 怎样计算 Ramp-up Period 时间?

    Ramp-up period 是指每个请求发生的总时间间隔, 单位是秒. 如果 Number of Threads 设置为5, 而 Ramp-up period 是10, 那么每个请求之间的间隔就是10/5, 也就是2秒, Ramp-up period 设置为0, 就是同时并发请求

12. 为什么 Aggregate Report 结果中的 Total 值不是真正的总和?

    JMeter 给结果中 Total 的定义并不完全指总和, 为了方便使用, 它的值表现了所在列的代表值, 比如 min 值, 它的 Total 就是所在列的最小值

13. 在运行结果中为何有 rate 为 N/A 的情况出现?

    可能因为 JMeter 自身问题造成, 再次运行可以得到正确结果

14. 在 使用 JMeter 测试时, 是完全模拟用户操作么? 造成的结果也和用户操作完全相同么?

    是的, JMeter 完全模拟用户操作, 所以操作记录会全部写入 DB. 在运行失败时, 可能会产生错误数据, 这就取决于脚本检查是否严谨, 否则错误数据也会进入 DB, 给程序运行带来很多麻烦

    小心缓存(类似查询接口压测, 先问问有没有做缓存)

    瓶颈处持续压测, 测试系统稳定性

    线上真实的一模一样的环境配置

    缓存洞穿, 持续压测/去缓存压测/有缓存压测

## 平均传输带宽

这个指标用于计算服务端的数据传输量。单位是KIB/S，1k字节也就是1024字节，KIB/S 就是每秒钟传输多少k字节。这个指标和我们上网的带宽是同一个指标。

# [公众号 性能测试如何入门](https://mp.weixin.qq.com/s?__biz=MzUyMTM4MzYyMw==&mid=2247484134&idx=1&sn=f1701638258f6169daa9a41e5d317353)

































