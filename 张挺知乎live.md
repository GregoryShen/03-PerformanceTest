# 张挺知乎live

## 引子-为什么要学性能测试

### 概述

1. 引子-为什么要学性能测试
2. 性能测试的基本原理
   1. Web性能测试的示意图
   2. 请求结果与监控结果
   3. 单次结果与统计结果
   4. 主要统计结果及其计算
3. 赶鸭子上架的性能测试怎么做
4. 性能测试场景设计策略
5. 赶鸭子上架的性能调优怎么做

### 为什么要学性能测试

1. 项目中往往不存在专职性能测试人员

2. 提高对系统的整体理解

3. 多维度思考系统质量,而非单纯功能角度

4. 转变自身角色

   我们作为测试人员并不仅仅是完成黑盒测试, 理解这个系统是怎样工作的

### 性能测试想做什么 - 尽可能模拟真实业务场景

UI泛指浏览器、移动app、桌面app、无图形界面的各种接口等

服务端泛指处理请求的服务器及他背后可能存在的很多很多服务器

上图是一个简化过的web性能测试的示意图,性能测试时尽可能用接近生产环境的专门环境来测试,即使如此仍然不能做到与生产环境完全相同,因此只能做估算和参考.

## 性能测试的基本原理

1. 模拟各种业务场景下的负载压力

2. 观察压力下服务端业务逻辑处理是否正常

3. 观察压力下服务端资源使用状况是否吃紧

   比如CPU使用率, 

4. 改变模拟的压力来重复上述观察

   可以改变小压力

一个简单的性能测试的基本流程:得到性能测试结果

性能测试结果由<u>请求结果</u>与<u>服务端监控结果</u>组成

<u>请求结果</u>包括响应时间、返回值等**<u>单次请求的响应里找到的结果</u>**以及他们对应的<u>统计结果</u>

请求结果: 通过解析服务端对请求的响应直接得到的结果, 如响应时间、响应的http状态码、响应体内容、单个响应包含的数据量等.此外还可以通过自定义逻辑间接得到一些复杂的请求结果.(比如一个无论如何都返回200度接口, 其真实成功与否由响应体内某字段决定)

请求结果是一个一个互相独立的结果, 每个请求有自己的请求结果

自定义的逻辑, 然后按照开发, 一些请求结果是要做一些比较复杂的逻辑运算的. 互相独立的结果. 比如测试发了1000个请求结果, 还有500个个是失败了. 这就是请求结果, 他是一个个互相独立的请求结果.

<u>服务端监控结果</u>包括<u>服务端各项资源使用率及数据量</u>等统计结果

服务端监控结果:一段时间内的CPU的负载, CPU使用率、网络数据传输量、内存的占用率、IO数据量等指标变化的情况

服务端监控结果是随着时间线不断变化的指标, 这一段时间内所有请求和响应共同造成了这个结果. 图中例子在11:30附近CPU占用率100%,表示到了性能瓶颈, 出图原理在时间线上各个时间点x坐标标识出对应的指标y坐标, 然后连成线, 所以图中11:00 到11:15中间由一小段同一个x上两个y上不可能的

### 单次结果与统计结果的区别举例:

**单次结果**: 调用正确/调用错误

**统计结果**: <u>持续调用n小时或n</u>次, 其中调用正确比例`x%`, 叫做<u>正确率</u>x%或错误率`(1-x%)`

以此类推, 单次结果的维度可以很多, 不局限于正确率错误率, 比如, 单次结果中返回的http代码是200, 404, 500, 对应的统计结果是 200比例x%, 404比例y%, 500比例z%, 要统计哪些单次结果?由具体业务决定, <u>统计结果还包括通过对统计结果进行计算得到的新的统计结果.</u>



### 主要统计结果及其计算

**(总)并发用户数**: 同一时间在系统上的用户数量, 这些用户可能分布在不同的功能模块或页面上

**(总)并发请求数**: 同一时间在系统上的用户同时向服务器作出的请求数量, 这些请求也可能分布在不同的功能模块或页面上.

同一时间一个用户可能只给系统发一个请求, 也可能给系统发100个请求, 所以, 下次有人告诉你系统要支持多少并发的时候, 问清楚上并发用户数还是并发请求数(这两个数字可能差很多)

**吞吐量(平均吞吐量)**: 吞吐量表示待测应用对业务的支持量, 以TPS或QPS为单位, 表示每秒钟能处理的请求数

**平均响应时间**: 一些请求从发起到收到服务端响应所需的时间的平均数

一段时间内的请求平均响应时间 = 单个请求的响应时间之和(秒) / 这段时间内服务端处理掉的总请求数(个)

一段时间内的平均吞吐量 = 这段时间内服务端处理掉的总请求数(个) / 时间(秒) 

> 单位名称是(TPS, transaction per second) 或 (QPS, Query per second)

设这段时间内服务端处理掉的总请求数(个)为n, 这段时间为t(秒), 一段时间内的请求平均响应时间为Tv(秒), 一段时间内的平均吞吐量为Qv, 单个响应的响应时间为Ts, SUM为求和函数. 则 

`Tv = SUM(Ts)/n` — 公式1

`Qv = n/t` — 公式2

由公式1与公式2推导出:

```
Qv = SUM(Ts)/Tv/t
```

`Qv = SUM(Ts)t/Tv` — 公式3

由公式3得: 平均吞吐量`Qv`与平均响应时间`Tv`成反比.

即: 平均响应时间越长, 则平均吞吐量越小.

这个推导是符合逻辑的, 当服务器硬件性能固定时:

1. 假设服务端只提供数据量极小的静态页面, 没有任何业务逻辑, 只需要处理0.1毫秒, 那么服务端可以每秒钟同时处理极大数量的请求, 那么吞吐量就高

2. 假设服务端收到每个请求都要处理1小时才给响应, 那么服务端每小时才能处理1个请求, 吞吐量就极低了.

## 赶鸭子上架的性能测试怎么做

## 性能测试场景设计策略

## 赶鸭子上架的性能调优怎么做

## 项目实例

* 某小型第三方支付平台压测
* 某第三方artifactory服务器上传下载大文件压测
* 某数据库百万级数据同步性能测试

